{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HExLQrE4ZxR"
   },
   "source": [
    "<h1><font color='blue'> 8E and 8F: Finding the Probability P(Y==1|X)</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LuKrFzC4ZxV"
   },
   "source": [
    "<h2><font color='Geen'> 8E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wES-wWN4ZxX"
   },
   "source": [
    "<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n",
    "\n",
    "Check the documentation for better understanding of these attributes: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "<img src='https://i.imgur.com/K11msU4.png' width=500>\n",
    "\n",
    "As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n",
    "\n",
    "Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n",
    "\n",
    "Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n",
    "\n",
    "Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n",
    "$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n",
    "\n",
    "RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n",
    "\n",
    "For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z830CfMk4Zxa"
   },
   "source": [
    "## Task E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuBxHiCQ4Zxc"
   },
   "source": [
    "> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n",
    "\n",
    "> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n",
    "\n",
    "> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fCgMNEvI4Zxf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ANUNIqCe4Zxn"
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHie1zqH4Zxt"
   },
   "source": [
    "### Pseudo code\n",
    "\n",
    "clf = SVC(gamma=0.001, C=100.)<br>\n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n",
    "   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n",
    "    \n",
    "fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n",
    "\n",
    "<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "h43kDT3M41u5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept value:  [4.97877929]\n",
      "Sample Output using inbuilt decision function  [-3.1600781   1.27373355 -3.38156398  2.06407708 -3.13163731 -2.70693083\n",
      " -3.52529898 -0.66210411  0.0965253  -4.10121003]\n",
      "Sample output using custom built decision function  [-3.1600781   1.27373355 -3.38156398  2.06407708 -3.13163731 -2.70693083\n",
      " -3.52529898 -0.66210411  0.0965253  -4.10121003]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, stratify=y, test_size=0.2)\n",
    "X_train, X_cv, Y_train, Y_cv = train_test_split(X_train, Y_train, stratify=Y_train, test_size=0.2)\n",
    "clf = SVC(gamma=0.001, C=100.)\n",
    "clf.fit(X_train,Y_train)\n",
    "coefficient_values = clf.dual_coef_[0]\n",
    "support_vectors = clf.support_vectors_\n",
    "intercept = clf.intercept_\n",
    "print('intercept value: ', intercept)\n",
    "\n",
    "svm_decision_inbuilt_result_x_cv = clf.decision_function(X_cv)\n",
    "print('Sample Output using inbuilt decision function ', svm_decision_inbuilt_result_x_cv[:10] )\n",
    "custom_decision_function_result_x_cv = decision_function(X_cv)\n",
    "print('Sample output using custom built decision function ', custom_decision_function_result_x_cv[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_function(Xcv):\n",
    "    sample_output_rbf = 0\n",
    "    y_predicted = []\n",
    "    gamma = clf._gamma\n",
    "  \n",
    "    for x_q in Xcv:\n",
    "        kernel_sum = 0\n",
    "        for i in range(len(support_vectors)):\n",
    "            squared_distance = (np.linalg.norm(support_vectors[i] - x_q)**2)\n",
    "            rbf_k = np.exp(-gamma * (squared_distance))\n",
    "            kernel_sum += coefficient_values[i]*rbf_k    \n",
    "        sample_output_rbf = kernel_sum + intercept\n",
    "        y_predicted.append(sample_output_rbf[0])\n",
    "    return np.array(y_predicted)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0bKCboN4Zxu"
   },
   "source": [
    "<h2><font color='Geen'> 8F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMn7OEN94Zxw"
   },
   "source": [
    "Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n",
    "<img src='https://i.imgur.com/CAMnVnh.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0n5EFkx4Zxz"
   },
   "source": [
    "## TASK F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0HOqVJq4Zx1"
   },
   "source": [
    "\n",
    "> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n",
    "\n",
    "> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n",
    "\n",
    "> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n",
    "<img src='https://i.imgur.com/zKYE9Oc.png'>\n",
    "if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n",
    "\n",
    "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(dim):\n",
    "    w = np.zeros_like(dim)\n",
    "    b = 0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(y_true, y_pred):\n",
    "    \n",
    "    len_y_true = len(y_true)\n",
    "    positive_points = np.count_nonzero(y_true == 1)\n",
    "    negative_points = np.count_nonzero(y_true == 0)\n",
    "    \n",
    "    y_plus = (positive_points+1)/(negative_points+2)\n",
    "    y_minus = 1/(negative_points+2)\n",
    "    \n",
    "    sum_of_loss = 0\n",
    "    \n",
    "    for i in range(0, len_y_true):\n",
    "        if (y_true[i] == 1):\n",
    "            sum_of_loss += ((y_plus * np.log10(y_pred[i])) + ((1- y_plus) * np.log10(1-y_pred[i])))\n",
    "        else:\n",
    "            sum_of_loss += ((y_minus * np.log10(y_pred[i])) + ((1 - y_minus) * np.log10(1-y_pred[i])))\n",
    "        \n",
    "    loss = (-1/len_y_true) * sum_of_loss  \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    z = np.dot(w, x) + b\n",
    "    dw = x*(y - sigmoid(z)) - ((alpha)*(1/N) * w)\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def gradient_db(x,y,w,b):\n",
    "    z = np.dot(w, x) + b\n",
    "    db = y - sigmoid(z)\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, X_test, y_test, epochs, alpha, eta0, tol=1e-3):\n",
    "    w, b = initialize_weights(X_train[0])\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    N = len(X_train)\n",
    "    loss_threshold = 0.0001\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(N - 1):\n",
    "            delta_weights = gradient_dw(X_train[i], y_train[i], w, b, alpha, len(X_train))\n",
    "            delta_bias = gradient_db(X_train[i], y_train[i], w, b)\n",
    "            w = w + eta0 * delta_weights\n",
    "            b = b + eta0 * delta_bias\n",
    "\n",
    "        y_prediction_train = [sigmoid(np.dot(w, x_i) + b) for x_i in X_train]\n",
    "        train_loss.append(logloss(y_train, y_prediction_train))\n",
    "        y_prediction_test = [sigmoid(np.dot(w, x_i) + b) for x_i in X_test]\n",
    "        test_loss.append(logloss(y_test, y_prediction_test))\n",
    "        \n",
    "        print(\n",
    "            f\"For EPOCH No : {epoch} Train Loss is : {logloss(y_train, y_prediction_train)} and Test Loss is : {logloss(y_test, y_prediction_test)}\"\n",
    "        )\n",
    "\n",
    "      \n",
    "\n",
    "    return w, b, train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For EPOCH No : 0 Train Loss is : 0.27236316193006377 and Test Loss is : [0.29923159 0.29934197 0.30130578 0.30199316 0.29130106]\n",
      "For EPOCH No : 1 Train Loss is : 0.2506189098774082 and Test Loss is : [0.29829924 0.29862947 0.30170201 0.30304284 0.28328607]\n",
      "For EPOCH No : 2 Train Loss is : 0.2340091119966072 and Test Loss is : [0.2979968  0.29861593 0.3021874  0.30414539 0.27662907]\n",
      "For EPOCH No : 3 Train Loss is : 0.22118875140192193 and Test Loss is : [0.29815084 0.29910012 0.3027382  0.30527776 0.27104635]\n",
      "For EPOCH No : 4 Train Loss is : 0.211179551986568 and Test Loss is : [0.29863637 0.29993861 0.30333672 0.30642445 0.26631794]\n",
      "For EPOCH No : 5 Train Loss is : 0.20327786347954666 and Test Loss is : [0.29936362 0.30102964 0.30396986 0.30757509 0.262275  ]\n",
      "For EPOCH No : 6 Train Loss is : 0.19697675012784463 and Test Loss is : [0.30026791 0.30230074 0.30462789 0.30872274 0.25878785]\n",
      "For EPOCH No : 7 Train Loss is : 0.19190790401046443 and Test Loss is : [0.30130216 0.30369991 0.30530356 0.30986279 0.25575629]\n",
      "For EPOCH No : 8 Train Loss is : 0.18780056527079508 and Test Loss is : [0.3024318  0.30518947 0.30599141 0.31099219 0.25310219]\n",
      "For EPOCH No : 9 Train Loss is : 0.1844530516717418 and Test Loss is : [0.30363117 0.30674182 0.30668732 0.31210895 0.25076397]\n",
      "For EPOCH No : 10 Train Loss is : 0.18171310530191265 and Test Loss is : [0.30488099 0.30833659 0.30738813 0.3132118  0.2486926 ]\n",
      "For EPOCH No : 11 Train Loss is : 0.1794642644381363 and Test Loss is : [0.30616667 0.30995858 0.30809143 0.3143     0.24684864]\n",
      "For EPOCH No : 12 Train Loss is : 0.17761632848907138 and Test Loss is : [0.30747707 0.31159639 0.30879535 0.31537315 0.24520001]\n",
      "For EPOCH No : 13 Train Loss is : 0.17609861184394895 and Test Loss is : [0.30880359 0.3132414  0.30949845 0.3164311  0.24372038]\n",
      "For EPOCH No : 14 Train Loss is : 0.17485511272003085 and Test Loss is : [0.31013959 0.31488707 0.31019961 0.31747385 0.24238797]\n",
      "For EPOCH No : 15 Train Loss is : 0.17384101039092317 and Test Loss is : [0.31147989 0.31652843 0.31089795 0.31850155 0.24118456]\n",
      "For EPOCH No : 16 Train Loss is : 0.17302009439769558 and Test Loss is : [0.31282045 0.31816169 0.3115928  0.31951442 0.24009486]\n",
      "For EPOCH No : 17 Train Loss is : 0.17236285538702745 and Test Loss is : [0.31415807 0.31978396 0.31228362 0.32051272 0.23910588]\n",
      "For EPOCH No : 18 Train Loss is : 0.1718450512172037 and Test Loss is : [0.31549028 0.32139307 0.31297002 0.32149678 0.23820655]\n",
      "For EPOCH No : 19 Train Loss is : 0.17144661842507927 and Test Loss is : [0.31681512 0.32298737 0.31365168 0.32246691 0.23738736]\n",
      "For EPOCH No : 20 Train Loss is : 0.17115083746597048 and Test Loss is : [0.31813106 0.32456565 0.31432836 0.32342348 0.23664011]\n",
      "For EPOCH No : 21 Train Loss is : 0.17094368642661345 and Test Loss is : [0.3194369  0.32612701 0.31499991 0.32436682 0.23595767]\n",
      "For EPOCH No : 22 Train Loss is : 0.1708133361432844 and Test Loss is : [0.32073172 0.32767082 0.31566618 0.3252973  0.23533382]\n",
      "For EPOCH No : 23 Train Loss is : 0.17074975243969406 and Test Loss is : [0.32201483 0.32919667 0.31632711 0.32621525 0.23476311]\n",
      "For EPOCH No : 24 Train Loss is : 0.17074438025700217 and Test Loss is : [0.32328567 0.3307043  0.31698263 0.32712102 0.23424075]\n",
      "For EPOCH No : 25 Train Loss is : 0.17078989093330232 and Test Loss is : [0.32454388 0.33219356 0.31763273 0.32801495 0.23376249]\n",
      "For EPOCH No : 26 Train Loss is : 0.17087997857941123 and Test Loss is : [0.32578916 0.33366443 0.31827739 0.32889736 0.23332457]\n",
      "For EPOCH No : 27 Train Loss is : 0.17100919492153913 and Test Loss is : [0.32702134 0.33511696 0.31891663 0.32976857 0.23292365]\n",
      "For EPOCH No : 28 Train Loss is : 0.17117281450399435 and Test Loss is : [0.3282403  0.33655125 0.31955047 0.33062888 0.23255671]\n",
      "For EPOCH No : 29 Train Loss is : 0.1713667240200611 and Test Loss is : [0.32944601 0.33796747 0.32017894 0.33147858 0.23222108]\n",
      "For EPOCH No : 30 Train Loss is : 0.17158733094450385 and Test Loss is : [0.33063847 0.33936581 0.32080211 0.33231798 0.23191435]\n",
      "For EPOCH No : 31 Train Loss is : 0.17183148770285545 and Test Loss is : [0.33181771 0.34074651 0.32142    0.33314733 0.23163432]\n",
      "For EPOCH No : 32 Train Loss is : 0.17209642842087874 and Test Loss is : [0.33298383 0.34210981 0.32203269 0.3339669  0.23137905]\n",
      "For EPOCH No : 33 Train Loss is : 0.17237971591734158 and Test Loss is : [0.33413691 0.34345599 0.32264024 0.33477695 0.23114673]\n",
      "For EPOCH No : 34 Train Loss is : 0.17267919708174087 and Test Loss is : [0.33527709 0.34478532 0.3232427  0.33557773 0.23093575]\n",
      "For EPOCH No : 35 Train Loss is : 0.1729929651504966 and Test Loss is : [0.33640451 0.3460981  0.32384015 0.33636947 0.23074463]\n",
      "For EPOCH No : 36 Train Loss is : 0.17331932768599276 and Test Loss is : [0.33751931 0.34739461 0.32443265 0.33715239 0.23057202]\n",
      "For EPOCH No : 37 Train Loss is : 0.17365677929167686 and Test Loss is : [0.33862168 0.34867516 0.32502028 0.33792672 0.23041669]\n",
      "For EPOCH No : 38 Train Loss is : 0.17400397827751576 and Test Loss is : [0.33971178 0.34994004 0.32560311 0.33869266 0.2302775 ]\n",
      "For EPOCH No : 39 Train Loss is : 0.17435972663418717 and Test Loss is : [0.34078978 0.35118955 0.3261812  0.33945042 0.23015342]\n",
      "For EPOCH No : 40 Train Loss is : 0.17472295278961716 and Test Loss is : [0.34185589 0.35242399 0.32675463 0.34020019 0.23004349]\n",
      "For EPOCH No : 41 Train Loss is : 0.17509269671412495 and Test Loss is : [0.34291027 0.35364366 0.32732347 0.34094216 0.22994683]\n",
      "For EPOCH No : 42 Train Loss is : 0.17546809701524946 and Test Loss is : [0.34395311 0.35484883 0.3278878  0.34167651 0.22986263]\n",
      "For EPOCH No : 43 Train Loss is : 0.17584837972404618 and Test Loss is : [0.34498462 0.3560398  0.32844767 0.34240342 0.22979014]\n",
      "For EPOCH No : 44 Train Loss is : 0.17623284852413576 and Test Loss is : [0.34600497 0.35721684 0.32900316 0.34312305 0.22972866]\n",
      "For EPOCH No : 45 Train Loss is : 0.17662087621527825 and Test Loss is : [0.34701435 0.35838024 0.32955434 0.34383556 0.22967754]\n",
      "For EPOCH No : 46 Train Loss is : 0.17701189723653407 and Test Loss is : [0.34801295 0.35953025 0.33010128 0.34454111 0.2296362 ]\n",
      "For EPOCH No : 47 Train Loss is : 0.1774054011015425 and Test Loss is : [0.34900095 0.36066715 0.33064403 0.34523985 0.22960406]\n",
      "For EPOCH No : 48 Train Loss is : 0.17780092662117392 and Test Loss is : [0.34997854 0.3617912  0.33118267 0.34593192 0.22958062]\n",
      "For EPOCH No : 49 Train Loss is : 0.17819805680772166 and Test Loss is : [0.3509459  0.36290263 0.33171727 0.34661747 0.22956538]\n",
      "w_coef  1.0997189432449603\n",
      "intercept b  -0.07567339899294767\n"
     ]
    }
   ],
   "source": [
    "alpha=0.0001\n",
    "eta0=0.0001\n",
    "epochs=50\n",
    "w, b, train_loss, test_loss = train(custom_decision_function_result_x_cv, Y_cv, X_test, Y_test, epochs, alpha, eta0)\n",
    "print('w_coef ', w)\n",
    "print('intercept b ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTY7z2bd4Zx2"
   },
   "source": [
    "__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.array([i for i in range(0, 50)])\n",
    "\n",
    "cv_log_loss_arr = np.array(train_loss)\n",
    "\n",
    "plt.plot(x, cv_log_loss_arr, label = 'CV log loss')\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid()\n",
    "\n",
    "plt.xlabel('Number of Epoch')\n",
    "plt.ylabel('Log Loss ')\n",
    "\n",
    "plt.title('Loss vs Epoch ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CM3odN1Z4Zx3"
   },
   "source": [
    "\n",
    "If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n",
    "\n",
    "1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n",
    "\n",
    "2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n",
    "\n",
    "3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n",
    "\n",
    "4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "8E&F_LR_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
